{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7857808e",
   "metadata": {},
   "source": [
    "# Wheat Disease Detection\n",
    "\n",
    "### 1.1 Introduction\n",
    "\n",
    "In this ipython notebook, we would be using various neural network models such as VGG19, Xception and InceptionV3 networks to train our wheat dataset. Implementing these networks using pre-trained weights for \"imagenet\" is very handy, especially for our computer vision problem of detecting diseases in wheat plants. \n",
    "\n",
    "In the first few cells, let us import and load the libraries that are important for predicting the changes of a wheat plant to suffer from a disease. Later, preprocessing of the image along with making them compatible with Keras would be done to ensure smooth pipeline. Finally, the networks mentioned above would be used for predicting whether the wheat in an image is healthy or diseased. \n",
    "\n",
    "### 1.2 Metrics\n",
    "\n",
    "1. Categorical Cross Entropy\n",
    "2. Accuracy \n",
    "\n",
    "### 1.3 Source \n",
    "\n",
    "The data was downloaded from https://drive.google.com/drive/folders/1OHKtwD1UrdmhqxrpQEeF_X_pqKotxRGD. \n",
    "\n",
    "It contains 4 folders with the name of the folder being the class. In each folder, there are quite a large number of wheat images and their condition. The following are the 4 classes which we would consider. \n",
    "\n",
    "1. Leaf Rust\n",
    "2. Crown and Root Rot\n",
    "3. Healthy Wheat\n",
    "4. Wheat Loose Smut\n",
    "\n",
    "The total number of images that we have taken into consideration are 4500 respectively. We would take about 20 percent of the points for testing using stratified sampling which you could see later in the notebook. \n",
    "\n",
    "### Table of Contents \n",
    "\n",
    "&emsp; 1.1 Introduction\n",
    "\n",
    "&emsp; 1.2 Metrics\n",
    "\n",
    "&emsp; 1.3 Source\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445c3b37",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "\n",
    "Below are some of the libraries that we are going to use for our deep learning application. These libraries make the task of training convolutional networks easy and straight-forward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5aebd99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import VGG19\n",
    "import cv2\n",
    "import os \n",
    "import pickle\n",
    "from imutils import paths\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.layers import Input, AveragePooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.applications import Xception, ResNet152, InceptionV3, NASNetLarge\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d1d0797",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = set([\"Crown and Root Rot\", \"Healthy Wheat\", \"Leaf Rust\", \"Wheat Loose Smut\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becea9ee",
   "metadata": {},
   "source": [
    "### Exploring the images and their paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b28124e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images/Crown and Root Rot/00041.jpg\n",
      "Images/Crown and Root Rot/00051.jpg\n",
      "Images/Crown and Root Rot/00061.jpg\n",
      "Images/Crown and Root Rot/00071.jpg\n",
      "Images/Crown and Root Rot/00081.png\n"
     ]
    }
   ],
   "source": [
    "count = 0 \n",
    "for images in paths.list_images('Images/Crown and Root Rot/'):\n",
    "    print(images)\n",
    "    count = count + 1\n",
    "    if count == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfb5665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('Images/Crown and Root Rot/00041.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = cv2.resize(image, (224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd7f8fa",
   "metadata": {},
   "source": [
    "Taking a look at the paths where the images are present so that they could be read later using opencv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c822c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images/Crown and Root Rot/00041.jpg\n",
      "Images/Crown and Root Rot/00051.jpg\n",
      "Images/Crown and Root Rot/00061.jpg\n",
      "Images/Crown and Root Rot/00071.jpg\n",
      "Images/Crown and Root Rot/00081.png\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for images in paths.list_images('Images/Crown and Root Rot/'):\n",
    "    print(images)\n",
    "    count = count + 1\n",
    "    if count == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c917e3",
   "metadata": {},
   "source": [
    "We find a way to get the name of the image to be used from paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1decd761",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = list(paths.list_images('Images/Crown and Root Rot/'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bbdb5e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Images/Crown and Root Rot/00041.jpg'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3f6db1",
   "metadata": {},
   "source": [
    "Let us now split the path to get our desired result of the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c749fb21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Images', 'Crown and Root Rot', '00041.jpg']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_path.split('/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4c68ba",
   "metadata": {},
   "source": [
    "When taking a look at the directory, we see the following files as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2417efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.gitattributes',\n",
       " '.ipynb_checkpoints',\n",
       " 'Images',\n",
       " 'README.md',\n",
       " 'Wheat Disease Detection.ipynb']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8db501a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels = os.listdir('Images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10155347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Crown and Root Rot', 'Healthy Wheat', 'Leaf Rust', 'Wheat Loose Smut']\n"
     ]
    }
   ],
   "source": [
    "print(Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "040a946c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CROWN_AND_ROOT_ROT_PATH = 'Images/Crown and Root Rot/'\n",
    "HEALTHY_AND_WHEAT_PATH = 'Images/Healthy Wheat/'\n",
    "LEAF_RUST_PATH = 'Images/Leaf Rust/'\n",
    "WHEAT_LOOSE_SMUT_PATH = 'Images/Wheat Loose Smut/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24776ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1021it [00:04, 247.20it/s]\n",
      "1146it [00:22, 51.50it/s] \n",
      "1286it [01:30, 14.15it/s] \n",
      "930it [00:04, 206.43it/s]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "labels = []\n",
    "for image_path in tqdm(paths.list_images(CROWN_AND_ROOT_ROT_PATH)):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    data.append(image)\n",
    "    \n",
    "    label = image_path.split('/')[-2]\n",
    "    \n",
    "    labels.append(label)\n",
    "    \n",
    "for image_path in tqdm(paths.list_images(HEALTHY_AND_WHEAT_PATH)):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    data.append(image)\n",
    "    \n",
    "    label = image_path.split('/')[-2]\n",
    "    \n",
    "    labels.append(label)\n",
    "\n",
    "for image_path in tqdm(paths.list_images(LEAF_RUST_PATH)):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    data.append(image)\n",
    "    \n",
    "    label = image_path.split('/')[-2]\n",
    "    \n",
    "    labels.append(label)\n",
    "    \n",
    "for image_path in tqdm(paths.list_images(WHEAT_LOOSE_SMUT_PATH)):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    data.append(image)\n",
    "    \n",
    "    label = image_path.split('/')[-2]\n",
    "    \n",
    "    labels.append(label)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b812a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wheat Loose Smut'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c47bc2",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "Transfer learning is a method in deep learning where a model that is developed for a particular task is reused as the starting point for our model so that we get it for our use cases. \n",
    "\n",
    "In the below cells, transfer learning is applied which ensures that models are not trained from the start. Instead only the last few layers for our application are trained. This leads to a good reduction in the time taken to train them. Furthermore, this leads to a good improvement in the accuracy of the model as state-of-the-art models are taken for training the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e00eff",
   "metadata": {},
   "source": [
    "## VGG19 Network\n",
    "\n",
    "VGG19 nework trained on imagenet weights is taken as our base model. In addition to this network, we add more layers so that the final model is used for our task of detecting and identifying wheat diseases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd528f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "headmodel = VGG19(weights = \"imagenet\", include_top = False,\n",
    "             input_tensor = Input(shape = (224, 224, 3)))\n",
    "model = headmodel.output\n",
    "model = AveragePooling2D(pool_size= (5, 5))(model)\n",
    "model = Flatten(name = 'flatten')(model)\n",
    "model = Dense(512, activation = 'relu')(model)\n",
    "model = Dropout(0.2)(model)\n",
    "model = Dense(len(Labels), activation = 'softmax')(model)\n",
    "\n",
    "final_model = Model(inputs = headmodel.input, outputs = model)\n",
    "\n",
    "for layer in headmodel.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "660dda78",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr = 1e-3)\n",
    "final_model.compile(loss = \"categorical_crossentropy\", optimizer = opt,\n",
    "                   metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9904c3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5af392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a67be207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4be9b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_cv, y_train, y_cv = train_test_split(data, labels, test_size = 0.2,\n",
    "                                                    stratify = labels, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1204c7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3506"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d03d2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16226382122578005790\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 14178494258108905959\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6922361920\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 17909503759591784877\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 2080, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 18243932917374272732\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e495498f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "55/55 [==============================] - 12s 227ms/step - loss: 1.5775 - accuracy: 0.6743 - val_loss: 0.6849 - val_accuracy: 0.8062\n",
      "Epoch 2/30\n",
      "55/55 [==============================] - 10s 190ms/step - loss: 0.4616 - accuracy: 0.8477 - val_loss: 0.4067 - val_accuracy: 0.8860\n",
      "Epoch 3/30\n",
      "55/55 [==============================] - 11s 199ms/step - loss: 0.2583 - accuracy: 0.9181 - val_loss: 0.2805 - val_accuracy: 0.9282\n",
      "Epoch 4/30\n",
      "55/55 [==============================] - 10s 191ms/step - loss: 0.1692 - accuracy: 0.9498 - val_loss: 0.2099 - val_accuracy: 0.9510\n",
      "Epoch 5/30\n",
      "55/55 [==============================] - 11s 194ms/step - loss: 0.1183 - accuracy: 0.9658 - val_loss: 0.1934 - val_accuracy: 0.9418\n",
      "Epoch 6/30\n",
      "55/55 [==============================] - 11s 196ms/step - loss: 0.1064 - accuracy: 0.9678 - val_loss: 0.1704 - val_accuracy: 0.9658\n",
      "Epoch 7/30\n",
      "55/55 [==============================] - 11s 199ms/step - loss: 0.0873 - accuracy: 0.9775 - val_loss: 0.1704 - val_accuracy: 0.9658\n",
      "Epoch 8/30\n",
      "55/55 [==============================] - 11s 207ms/step - loss: 0.0764 - accuracy: 0.9826 - val_loss: 0.1989 - val_accuracy: 0.9544\n",
      "Epoch 9/30\n",
      "55/55 [==============================] - 11s 204ms/step - loss: 0.0706 - accuracy: 0.9815 - val_loss: 0.2149 - val_accuracy: 0.9441\n",
      "Epoch 10/30\n",
      "55/55 [==============================] - 11s 203ms/step - loss: 0.0814 - accuracy: 0.9792 - val_loss: 0.1550 - val_accuracy: 0.9658\n",
      "Epoch 11/30\n",
      "55/55 [==============================] - 11s 208ms/step - loss: 0.0592 - accuracy: 0.9837 - val_loss: 0.1827 - val_accuracy: 0.9635\n",
      "Epoch 12/30\n",
      "55/55 [==============================] - 12s 210ms/step - loss: 0.0711 - accuracy: 0.9826 - val_loss: 0.1713 - val_accuracy: 0.9704\n",
      "Epoch 13/30\n",
      "55/55 [==============================] - 12s 211ms/step - loss: 0.0628 - accuracy: 0.9835 - val_loss: 0.1632 - val_accuracy: 0.9647\n",
      "Epoch 14/30\n",
      "55/55 [==============================] - 12s 212ms/step - loss: 0.0553 - accuracy: 0.9855 - val_loss: 0.1682 - val_accuracy: 0.9647\n",
      "Epoch 15/30\n",
      "55/55 [==============================] - 12s 219ms/step - loss: 0.0482 - accuracy: 0.9860 - val_loss: 0.1677 - val_accuracy: 0.9647\n",
      "Epoch 16/30\n",
      "55/55 [==============================] - 13s 229ms/step - loss: 0.0559 - accuracy: 0.9872 - val_loss: 0.1641 - val_accuracy: 0.9612\n",
      "Epoch 17/30\n",
      "55/55 [==============================] - 12s 223ms/step - loss: 0.0488 - accuracy: 0.9863 - val_loss: 0.1697 - val_accuracy: 0.9612- loss: 0.049\n",
      "Epoch 18/30\n",
      "55/55 [==============================] - 13s 233ms/step - loss: 0.0410 - accuracy: 0.9892 - val_loss: 0.1602 - val_accuracy: 0.9658\n",
      "Epoch 19/30\n",
      "55/55 [==============================] - 13s 234ms/step - loss: 0.0579 - accuracy: 0.9883 - val_loss: 0.1491 - val_accuracy: 0.9669\n",
      "Epoch 20/30\n",
      "55/55 [==============================] - 13s 233ms/step - loss: 0.0460 - accuracy: 0.9883 - val_loss: 0.1651 - val_accuracy: 0.96470466 - accu - ETA: 0s - loss: 0.0471 - accura\n",
      "Epoch 21/30\n",
      "55/55 [==============================] - 13s 231ms/step - loss: 0.0453 - accuracy: 0.9855 - val_loss: 0.1824 - val_accuracy: 0.9555\n",
      "Epoch 22/30\n",
      "55/55 [==============================] - 13s 234ms/step - loss: 0.0421 - accuracy: 0.9897 - val_loss: 0.1657 - val_accuracy: 0.9658\n",
      "Epoch 23/30\n",
      "55/55 [==============================] - 13s 237ms/step - loss: 0.0399 - accuracy: 0.9892 - val_loss: 0.1899 - val_accuracy: 0.9624\n",
      "Epoch 24/30\n",
      "55/55 [==============================] - 13s 236ms/step - loss: 0.0493 - accuracy: 0.9855 - val_loss: 0.2036 - val_accuracy: 0.9590\n",
      "Epoch 25/30\n",
      "55/55 [==============================] - 13s 239ms/step - loss: 0.0436 - accuracy: 0.9886 - val_loss: 0.1743 - val_accuracy: 0.9647\n",
      "Epoch 26/30\n",
      "55/55 [==============================] - 13s 238ms/step - loss: 0.0492 - accuracy: 0.9886 - val_loss: 0.1513 - val_accuracy: 0.9658: 0.0 - ETA: 5s - loss: 0.0232 - accura - ETA: 0s - loss: 0.0476 - accuracy: \n",
      "Epoch 27/30\n",
      "55/55 [==============================] - 14s 250ms/step - loss: 0.0534 - accuracy: 0.9866 - val_loss: 0.1332 - val_accuracy: 0.9669\n",
      "Epoch 28/30\n",
      "55/55 [==============================] - 13s 242ms/step - loss: 0.0363 - accuracy: 0.9897 - val_loss: 0.1678 - val_accuracy: 0.9692\n",
      "Epoch 29/30\n",
      "55/55 [==============================] - 13s 243ms/step - loss: 0.0299 - accuracy: 0.9892 - val_loss: 0.1595 - val_accuracy: 0.9704\n",
      "Epoch 30/30\n",
      "55/55 [==============================] - 13s 241ms/step - loss: 0.0429 - accuracy: 0.9875 - val_loss: 0.1566 - val_accuracy: 0.9647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2984b23a040>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.fit(X_train, y_train, validation_data = (X_cv, y_cv), \n",
    "               epochs = 30, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4f92397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99d6e5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1021it [00:04, 227.64it/s]\n",
      "1146it [00:24, 46.33it/s] \n",
      "1286it [01:36, 13.31it/s] \n",
      "930it [00:04, 192.48it/s]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "labels = []\n",
    "for image_path in tqdm(paths.list_images(CROWN_AND_ROOT_ROT_PATH)):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (299, 299))\n",
    "    data.append(image)\n",
    "    \n",
    "    label = image_path.split('/')[-2]\n",
    "    \n",
    "    labels.append(label)\n",
    "    \n",
    "for image_path in tqdm(paths.list_images(HEALTHY_AND_WHEAT_PATH)):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (299, 299))\n",
    "    data.append(image)\n",
    "    \n",
    "    label = image_path.split('/')[-2]\n",
    "    \n",
    "    labels.append(label)\n",
    "\n",
    "for image_path in tqdm(paths.list_images(LEAF_RUST_PATH)):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (299, 299))\n",
    "    data.append(image)\n",
    "    \n",
    "    label = image_path.split('/')[-2]\n",
    "    \n",
    "    labels.append(label)\n",
    "    \n",
    "for image_path in tqdm(paths.list_images(WHEAT_LOOSE_SMUT_PATH)):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (299, 299))\n",
    "    data.append(image)\n",
    "    \n",
    "    label = image_path.split('/')[-2]\n",
    "    \n",
    "    labels.append(label)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "91e98678",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "10e5d92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_cv, y_train, y_cv = train_test_split(data, labels, test_size = 0.2,\n",
    "                                                    stratify = labels, random_state = 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef8f271",
   "metadata": {},
   "source": [
    "## Xception Network\n",
    "\n",
    "It is now time to train an xception network to see how the training accuracy and the cross-validation accuracy increases with increase in the number of epochs.\n",
    "\n",
    "As could be seen below from the architecture, the input shape that is accepted by Xception network is (299, 299, 3). Therefore, we did a good thing in the above cells in resizing the images to be of (299, 299) shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eef321bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "headmodel = Xception(weights = \"imagenet\", include_top = False,\n",
    "             input_tensor = Input(shape = (299, 299, 3)))\n",
    "model = headmodel.output\n",
    "model = AveragePooling2D(pool_size= (5, 5))(model)\n",
    "model = Flatten(name = 'flatten')(model)\n",
    "model = Dense(512, activation = 'relu')(model)\n",
    "model = Dropout(0.2)(model)\n",
    "model = Dense(len(Labels), activation = 'softmax')(model)\n",
    "\n",
    "final_model = Model(inputs = headmodel.input, outputs = model)\n",
    "\n",
    "for layer in headmodel.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b94ed951",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr = 1e-3)\n",
    "final_model.compile(loss = \"categorical_crossentropy\", optimizer = opt,\n",
    "                   metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bdf0a0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "55/55 [==============================] - 17s 305ms/step - loss: 14.1744 - accuracy: 0.4820 - val_loss: 1.1539 - val_accuracy: 0.5553\n",
      "Epoch 2/30\n",
      "55/55 [==============================] - 15s 280ms/step - loss: 0.9662 - accuracy: 0.6081 - val_loss: 0.8232 - val_accuracy: 0.6682\n",
      "Epoch 3/30\n",
      "55/55 [==============================] - 16s 283ms/step - loss: 0.7972 - accuracy: 0.6680 - val_loss: 0.7505 - val_accuracy: 0.6967\n",
      "Epoch 4/30\n",
      "55/55 [==============================] - 16s 286ms/step - loss: 0.7010 - accuracy: 0.7108 - val_loss: 0.6783 - val_accuracy: 0.7161\n",
      "Epoch 5/30\n",
      "55/55 [==============================] - 16s 288ms/step - loss: 0.5677 - accuracy: 0.7687 - val_loss: 0.5752 - val_accuracy: 0.7685\n",
      "Epoch 6/30\n",
      "55/55 [==============================] - 16s 297ms/step - loss: 0.4911 - accuracy: 0.8075 - val_loss: 0.5500 - val_accuracy: 0.8062\n",
      "Epoch 7/30\n",
      "55/55 [==============================] - 17s 312ms/step - loss: 0.4465 - accuracy: 0.8257 - val_loss: 0.5043 - val_accuracy: 0.8210\n",
      "Epoch 8/30\n",
      "55/55 [==============================] - 17s 314ms/step - loss: 0.4195 - accuracy: 0.8283 - val_loss: 0.4364 - val_accuracy: 0.8312\n",
      "Epoch 9/30\n",
      "55/55 [==============================] - 18s 325ms/step - loss: 0.3481 - accuracy: 0.8685 - val_loss: 0.4317 - val_accuracy: 0.8472\n",
      "Epoch 10/30\n",
      "55/55 [==============================] - 18s 334ms/step - loss: 0.3183 - accuracy: 0.8762 - val_loss: 0.4478 - val_accuracy: 0.8324\n",
      "Epoch 11/30\n",
      "55/55 [==============================] - 19s 338ms/step - loss: 0.3414 - accuracy: 0.8662 - val_loss: 0.3838 - val_accuracy: 0.8666\n",
      "Epoch 12/30\n",
      "55/55 [==============================] - 19s 341ms/step - loss: 0.3025 - accuracy: 0.8813 - val_loss: 0.4515 - val_accuracy: 0.8563\n",
      "Epoch 13/30\n",
      "55/55 [==============================] - 19s 343ms/step - loss: 0.2774 - accuracy: 0.8942 - val_loss: 0.3464 - val_accuracy: 0.8974\n",
      "Epoch 14/30\n",
      "55/55 [==============================] - 20s 358ms/step - loss: 0.2701 - accuracy: 0.8959 - val_loss: 0.3095 - val_accuracy: 0.8928\n",
      "Epoch 15/30\n",
      "55/55 [==============================] - 19s 353ms/step - loss: 0.2537 - accuracy: 0.9107 - val_loss: 0.3099 - val_accuracy: 0.8962\n",
      "Epoch 16/30\n",
      "55/55 [==============================] - 20s 362ms/step - loss: 0.2466 - accuracy: 0.9039 - val_loss: 0.3075 - val_accuracy: 0.8871\n",
      "Epoch 17/30\n",
      "55/55 [==============================] - 20s 367ms/step - loss: 0.2129 - accuracy: 0.9201 - val_loss: 0.3335 - val_accuracy: 0.8837\n",
      "Epoch 18/30\n",
      "55/55 [==============================] - 19s 354ms/step - loss: 0.2361 - accuracy: 0.9104 - val_loss: 0.3225 - val_accuracy: 0.8894\n",
      "Epoch 19/30\n",
      "55/55 [==============================] - 20s 365ms/step - loss: 0.2536 - accuracy: 0.8985 - val_loss: 0.3313 - val_accuracy: 0.8826\n",
      "Epoch 20/30\n",
      "55/55 [==============================] - 20s 359ms/step - loss: 0.1890 - accuracy: 0.9250 - val_loss: 0.3152 - val_accuracy: 0.8951\n",
      "Epoch 21/30\n",
      "55/55 [==============================] - 20s 363ms/step - loss: 0.2018 - accuracy: 0.9244 - val_loss: 0.3108 - val_accuracy: 0.9042\n",
      "Epoch 22/30\n",
      "55/55 [==============================] - 21s 373ms/step - loss: 0.1598 - accuracy: 0.9373 - val_loss: 0.3372 - val_accuracy: 0.9122\n",
      "Epoch 23/30\n",
      "55/55 [==============================] - 20s 371ms/step - loss: 0.1869 - accuracy: 0.9290 - val_loss: 0.3130 - val_accuracy: 0.8997\n",
      "Epoch 24/30\n",
      "55/55 [==============================] - 20s 361ms/step - loss: 0.2237 - accuracy: 0.9176 - val_loss: 0.3956 - val_accuracy: 0.8848\n",
      "Epoch 25/30\n",
      "55/55 [==============================] - 20s 367ms/step - loss: 0.2035 - accuracy: 0.9122 - val_loss: 0.3486 - val_accuracy: 0.9031\n",
      "Epoch 26/30\n",
      "55/55 [==============================] - 20s 369ms/step - loss: 0.1874 - accuracy: 0.9290 - val_loss: 0.2999 - val_accuracy: 0.9019\n",
      "Epoch 27/30\n",
      "55/55 [==============================] - 20s 367ms/step - loss: 0.1862 - accuracy: 0.9258 - val_loss: 0.2977 - val_accuracy: 0.9213\n",
      "Epoch 28/30\n",
      "55/55 [==============================] - 20s 362ms/step - loss: 0.1695 - accuracy: 0.9284 - val_loss: 0.3001 - val_accuracy: 0.9168\n",
      "Epoch 29/30\n",
      "55/55 [==============================] - 20s 363ms/step - loss: 0.1438 - accuracy: 0.9418 - val_loss: 0.3013 - val_accuracy: 0.9361\n",
      "Epoch 30/30\n",
      "55/55 [==============================] - 20s 369ms/step - loss: 0.1779 - accuracy: 0.9290 - val_loss: 0.2984 - val_accuracy: 0.9202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29a62a234f0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.fit(X_train, y_train, validation_data = (X_cv, y_cv), \n",
    "               epochs = 30, batch_size = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9892a076",
   "metadata": {},
   "source": [
    "## InceptionV3 Network\n",
    "\n",
    "In the below cells, InceptionV3 network would be implemented with the weights taken from imagenet data. Inception weights are not trained and additional layers that are added on top of inception are trained to help us get the output for our prediction of disease in wheat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a700b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "headmodel = InceptionV3(weights = \"imagenet\", include_top = False,\n",
    "             input_tensor = Input(shape = (299, 299, 3)))\n",
    "model = headmodel.output\n",
    "model = AveragePooling2D(pool_size= (5, 5))(model)\n",
    "model = Flatten(name = 'flatten')(model)\n",
    "model = Dense(512, activation = 'relu')(model)\n",
    "model = Dense(256, activation = 'relu')(model)\n",
    "model = Dropout(0.1)(model)\n",
    "model = Dense(len(Labels), activation = 'softmax')(model)\n",
    "\n",
    "final_model = Model(inputs = headmodel.input, outputs = model)\n",
    "\n",
    "for layer in headmodel.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "97acd01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr = 1e-3)\n",
    "final_model.compile(loss = \"categorical_crossentropy\", optimizer = opt,\n",
    "                   metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c6f2af10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "55/55 [==============================] - 9s 172ms/step - loss: 7.3515 - accuracy: 0.3582 - val_loss: 1.5071 - val_accuracy: 0.4493\n",
      "Epoch 2/30\n",
      "55/55 [==============================] - 9s 157ms/step - loss: 1.2283 - accuracy: 0.5003 - val_loss: 1.1527 - val_accuracy: 0.5450\n",
      "Epoch 3/30\n",
      "55/55 [==============================] - 9s 168ms/step - loss: 1.0291 - accuracy: 0.5804 - val_loss: 1.1947 - val_accuracy: 0.5336\n",
      "Epoch 4/30\n",
      "55/55 [==============================] - 10s 188ms/step - loss: 0.9762 - accuracy: 0.6067 - val_loss: 1.0577 - val_accuracy: 0.5747\n",
      "Epoch 5/30\n",
      "55/55 [==============================] - 10s 173ms/step - loss: 0.9062 - accuracy: 0.6349 - val_loss: 1.0600 - val_accuracy: 0.5895\n",
      "Epoch 6/30\n",
      "55/55 [==============================] - 10s 180ms/step - loss: 0.8292 - accuracy: 0.6711 - val_loss: 1.0552 - val_accuracy: 0.6100\n",
      "Epoch 7/30\n",
      "55/55 [==============================] - 10s 183ms/step - loss: 0.7476 - accuracy: 0.7071 - val_loss: 0.8938 - val_accuracy: 0.6477\n",
      "Epoch 8/30\n",
      "55/55 [==============================] - 10s 186ms/step - loss: 0.7116 - accuracy: 0.7196 - val_loss: 0.9057 - val_accuracy: 0.6591\n",
      "Epoch 9/30\n",
      "55/55 [==============================] - 11s 196ms/step - loss: 0.6748 - accuracy: 0.7250 - val_loss: 0.8167 - val_accuracy: 0.6899\n",
      "Epoch 10/30\n",
      "55/55 [==============================] - 10s 189ms/step - loss: 0.6083 - accuracy: 0.7721 - val_loss: 0.9131 - val_accuracy: 0.6534\n",
      "Epoch 11/30\n",
      "55/55 [==============================] - 10s 189ms/step - loss: 0.6524 - accuracy: 0.7530 - val_loss: 0.7350 - val_accuracy: 0.7320\n",
      "Epoch 12/30\n",
      "55/55 [==============================] - 10s 189ms/step - loss: 0.5679 - accuracy: 0.7801 - val_loss: 0.7225 - val_accuracy: 0.7081\n",
      "Epoch 13/30\n",
      "55/55 [==============================] - 11s 191ms/step - loss: 0.5290 - accuracy: 0.8006 - val_loss: 0.6698 - val_accuracy: 0.7526\n",
      "Epoch 14/30\n",
      "55/55 [==============================] - 10s 190ms/step - loss: 0.4777 - accuracy: 0.8220 - val_loss: 0.5740 - val_accuracy: 0.7777\n",
      "Epoch 15/30\n",
      "55/55 [==============================] - 10s 190ms/step - loss: 0.4189 - accuracy: 0.8454 - val_loss: 0.6846 - val_accuracy: 0.7491\n",
      "Epoch 16/30\n",
      "55/55 [==============================] - 11s 191ms/step - loss: 0.4025 - accuracy: 0.8537 - val_loss: 0.6037 - val_accuracy: 0.8062\n",
      "Epoch 17/30\n",
      "55/55 [==============================] - 11s 192ms/step - loss: 0.3524 - accuracy: 0.8685 - val_loss: 0.5679 - val_accuracy: 0.8016\n",
      "Epoch 18/30\n",
      "55/55 [==============================] - 11s 193ms/step - loss: 0.3152 - accuracy: 0.8868 - val_loss: 0.4975 - val_accuracy: 0.8461\n",
      "Epoch 19/30\n",
      "55/55 [==============================] - 11s 193ms/step - loss: 0.2954 - accuracy: 0.8936 - val_loss: 0.4807 - val_accuracy: 0.8335\n",
      "Epoch 20/30\n",
      "55/55 [==============================] - 11s 192ms/step - loss: 0.2757 - accuracy: 0.9042 - val_loss: 0.5318 - val_accuracy: 0.8301\n",
      "Epoch 21/30\n",
      "55/55 [==============================] - 11s 194ms/step - loss: 0.2470 - accuracy: 0.9096 - val_loss: 0.4797 - val_accuracy: 0.8438\n",
      "Epoch 22/30\n",
      "55/55 [==============================] - 11s 192ms/step - loss: 0.2238 - accuracy: 0.9216 - val_loss: 0.4550 - val_accuracy: 0.8586\n",
      "Epoch 23/30\n",
      "55/55 [==============================] - 11s 193ms/step - loss: 0.2759 - accuracy: 0.8928 - val_loss: 0.6163 - val_accuracy: 0.8096\n",
      "Epoch 24/30\n",
      "55/55 [==============================] - 11s 194ms/step - loss: 0.2856 - accuracy: 0.8936 - val_loss: 0.4676 - val_accuracy: 0.8632\n",
      "Epoch 25/30\n",
      "55/55 [==============================] - 11s 192ms/step - loss: 0.2656 - accuracy: 0.9005 - val_loss: 0.6898 - val_accuracy: 0.7742\n",
      "Epoch 26/30\n",
      "55/55 [==============================] - 11s 198ms/step - loss: 0.2981 - accuracy: 0.8908 - val_loss: 0.4464 - val_accuracy: 0.8700\n",
      "Epoch 27/30\n",
      "55/55 [==============================] - 11s 196ms/step - loss: 0.2204 - accuracy: 0.9244 - val_loss: 0.4851 - val_accuracy: 0.8415\n",
      "Epoch 28/30\n",
      "55/55 [==============================] - 11s 192ms/step - loss: 0.2720 - accuracy: 0.8987 - val_loss: 0.5030 - val_accuracy: 0.8620\n",
      "Epoch 29/30\n",
      "55/55 [==============================] - 11s 192ms/step - loss: 0.2032 - accuracy: 0.9318 - val_loss: 0.5319 - val_accuracy: 0.8381\n",
      "Epoch 30/30\n",
      "55/55 [==============================] - 11s 193ms/step - loss: 0.1827 - accuracy: 0.9338 - val_loss: 0.4858 - val_accuracy: 0.8769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29a6bf03340>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.fit(X_train, y_train, validation_data = (X_cv, y_cv), \n",
    "               epochs = 30, batch_size = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77021f78",
   "metadata": {},
   "source": [
    "## ResNet152 Network\n",
    "\n",
    "ResNet152 is used to check the performance of the models. Sometimes we are under the assumption that adding more layers in the neural network would reduce the training and test loss.\n",
    "\n",
    "According to experiments, however, we see that adding more layers in our network would not decrease the training loss to a large extent.\n",
    "\n",
    "Below is the implementation of ResNet152 Network. Feel free to take a look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d982ff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "headmodel = ResNet152(weights = \"imagenet\", include_top = False,\n",
    "             input_tensor = Input(shape = (299, 299, 3)))\n",
    "model = headmodel.output\n",
    "model = AveragePooling2D(pool_size= (5, 5))(model)\n",
    "model = Flatten(name = 'flatten')(model)\n",
    "model = Dense(512, activation = 'relu')(model)\n",
    "model = Dropout(0.1)(model)\n",
    "model = Dense(len(Labels), activation = 'softmax')(model)\n",
    "\n",
    "final_model = Model(inputs = headmodel.input, outputs = model)\n",
    "\n",
    "for layer in headmodel.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f3299a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr = 1e-2)\n",
    "final_model.compile(loss = \"categorical_crossentropy\", optimizer = opt,\n",
    "                   metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4897ba2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "55/55 [==============================] - 35s 636ms/step - loss: 15.4674 - accuracy: 0.6013 - val_loss: 0.5931 - val_accuracy: 0.8096\n",
      "Epoch 2/30\n",
      "55/55 [==============================] - 35s 632ms/step - loss: 0.4778 - accuracy: 0.8080 - val_loss: 0.3877 - val_accuracy: 0.8666\n",
      "Epoch 3/30\n",
      "55/55 [==============================] - 36s 659ms/step - loss: 0.3131 - accuracy: 0.8791 - val_loss: 0.2539 - val_accuracy: 0.9054\n",
      "Epoch 4/30\n",
      "55/55 [==============================] - 37s 680ms/step - loss: 0.2377 - accuracy: 0.9064 - val_loss: 0.2702 - val_accuracy: 0.9111\n",
      "Epoch 5/30\n",
      "55/55 [==============================] - 39s 708ms/step - loss: 0.1797 - accuracy: 0.9301 - val_loss: 0.1852 - val_accuracy: 0.9327\n",
      "Epoch 6/30\n",
      "55/55 [==============================] - 38s 687ms/step - loss: 0.1664 - accuracy: 0.9301 - val_loss: 0.1746 - val_accuracy: 0.9384\n",
      "Epoch 7/30\n",
      "55/55 [==============================] - 38s 687ms/step - loss: 0.1296 - accuracy: 0.9469 - val_loss: 0.1716 - val_accuracy: 0.9373\n",
      "Epoch 8/30\n",
      "55/55 [==============================] - 38s 687ms/step - loss: 0.1177 - accuracy: 0.9518 - val_loss: 0.2048 - val_accuracy: 0.9384\n",
      "Epoch 9/30\n",
      "55/55 [==============================] - 39s 707ms/step - loss: 0.1023 - accuracy: 0.9598 - val_loss: 0.1578 - val_accuracy: 0.9396\n",
      "Epoch 10/30\n",
      "55/55 [==============================] - 38s 694ms/step - loss: 0.0975 - accuracy: 0.9592 - val_loss: 0.1973 - val_accuracy: 0.9475\n",
      "Epoch 11/30\n",
      "55/55 [==============================] - 39s 707ms/step - loss: 0.0903 - accuracy: 0.9606 - val_loss: 0.1619 - val_accuracy: 0.9544\n",
      "Epoch 12/30\n",
      "55/55 [==============================] - 38s 697ms/step - loss: 0.0909 - accuracy: 0.9615 - val_loss: 0.1831 - val_accuracy: 0.9487\n",
      "Epoch 13/30\n",
      "55/55 [==============================] - 38s 693ms/step - loss: 0.0969 - accuracy: 0.9621 - val_loss: 0.1693 - val_accuracy: 0.9453\n",
      "Epoch 14/30\n",
      "55/55 [==============================] - 39s 705ms/step - loss: 0.0889 - accuracy: 0.9615 - val_loss: 0.2493 - val_accuracy: 0.9441\n",
      "Epoch 15/30\n",
      "55/55 [==============================] - 38s 698ms/step - loss: 0.0937 - accuracy: 0.9655 - val_loss: 0.1622 - val_accuracy: 0.9567\n",
      "Epoch 16/30\n",
      "55/55 [==============================] - 39s 704ms/step - loss: 0.1253 - accuracy: 0.9495 - val_loss: 0.2420 - val_accuracy: 0.9373\n",
      "Epoch 17/30\n",
      "55/55 [==============================] - 41s 739ms/step - loss: 0.1170 - accuracy: 0.9527 - val_loss: 0.2616 - val_accuracy: 0.9441\n",
      "Epoch 18/30\n",
      "55/55 [==============================] - 39s 706ms/step - loss: 0.1090 - accuracy: 0.9586 - val_loss: 0.2603 - val_accuracy: 0.9384\n",
      "Epoch 19/30\n",
      "55/55 [==============================] - 39s 707ms/step - loss: 0.0992 - accuracy: 0.9578 - val_loss: 0.2036 - val_accuracy: 0.9430\n",
      "Epoch 20/30\n",
      "55/55 [==============================] - 39s 704ms/step - loss: 0.0885 - accuracy: 0.9638 - val_loss: 0.1937 - val_accuracy: 0.9464\n",
      "Epoch 21/30\n",
      "55/55 [==============================] - 39s 717ms/step - loss: 0.1270 - accuracy: 0.9549 - val_loss: 0.3095 - val_accuracy: 0.9361\n",
      "Epoch 22/30\n",
      "55/55 [==============================] - 39s 705ms/step - loss: 0.1017 - accuracy: 0.9612 - val_loss: 0.2888 - val_accuracy: 0.9384\n",
      "Epoch 23/30\n",
      "55/55 [==============================] - 38s 700ms/step - loss: 0.0865 - accuracy: 0.9663 - val_loss: 0.2781 - val_accuracy: 0.9475\n",
      "Epoch 24/30\n",
      "55/55 [==============================] - 39s 700ms/step - loss: 0.0846 - accuracy: 0.9681 - val_loss: 0.1820 - val_accuracy: 0.9453\n",
      "Epoch 25/30\n",
      "55/55 [==============================] - 40s 726ms/step - loss: 0.0728 - accuracy: 0.9703 - val_loss: 0.2093 - val_accuracy: 0.9544\n",
      "Epoch 26/30\n",
      "55/55 [==============================] - 39s 717ms/step - loss: 0.0612 - accuracy: 0.9752 - val_loss: 0.2743 - val_accuracy: 0.9498\n",
      "Epoch 27/30\n",
      "55/55 [==============================] - 39s 711ms/step - loss: 0.0829 - accuracy: 0.9672 - val_loss: 0.2755 - val_accuracy: 0.9350\n",
      "Epoch 28/30\n",
      "55/55 [==============================] - 40s 733ms/step - loss: 0.0868 - accuracy: 0.9649 - val_loss: 0.2376 - val_accuracy: 0.9532\n",
      "Epoch 29/30\n",
      "55/55 [==============================] - 38s 697ms/step - loss: 0.0955 - accuracy: 0.9669 - val_loss: 0.2899 - val_accuracy: 0.9441\n",
      "Epoch 30/30\n",
      "55/55 [==============================] - 38s 700ms/step - loss: 0.1472 - accuracy: 0.9487 - val_loss: 0.3551 - val_accuracy: 0.9179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29a74ce5e80>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.fit(X_train, y_train, validation_data = (X_cv, y_cv), \n",
    "               epochs = 30, batch_size = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6390315",
   "metadata": {},
   "source": [
    "## NasNetLarge Network\n",
    "\n",
    "It is now time to experiment with NasNet architecture and evaluation the performance of the model respectively. Going through the architectures of the models is really handy, especially to ensure that one learns where they are used and the training and optimization of weights is taking place. Therefore, you could go through the documentation of NasNetLarge to get to understand the architecture of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f04a8422",
   "metadata": {},
   "outputs": [],
   "source": [
    "headmodel = NASNetLarge(weights = \"imagenet\", include_top = False,\n",
    "             input_tensor = Input(shape = (299, 299, 3)))\n",
    "model = headmodel.output\n",
    "model = AveragePooling2D(pool_size= (5, 5))(model)\n",
    "model = Flatten(name = 'flatten')(model)\n",
    "# model = Dense(512, activation = 'relu')(model)\n",
    "model = Dense(256, activation = 'relu')(model)\n",
    "model = Dropout(0.2)(model)\n",
    "model = Dense(len(Labels), activation = 'softmax')(model)\n",
    "\n",
    "final_model = Model(inputs = headmodel.input, outputs = model)\n",
    "\n",
    "for layer in headmodel.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3272198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr = 1e-3)\n",
    "final_model.compile(loss = \"categorical_crossentropy\", optimizer = opt,\n",
    "                   metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517afc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "55/55 [==============================] - 64s 1s/step - loss: 2.2587 - accuracy: 0.4432 - val_loss: 1.0733 - val_accuracy: 0.5450\n",
      "Epoch 2/30\n",
      "55/55 [==============================] - 71s 1s/step - loss: 1.0516 - accuracy: 0.5602 - val_loss: 1.0400 - val_accuracy: 0.5587\n",
      "Epoch 3/30\n",
      "55/55 [==============================] - 73s 1s/step - loss: 0.9723 - accuracy: 0.6050 - val_loss: 0.9456 - val_accuracy: 0.6055\n",
      "Epoch 4/30\n",
      "55/55 [==============================] - 73s 1s/step - loss: 0.9254 - accuracy: 0.6201 - val_loss: 0.9527 - val_accuracy: 0.6021\n",
      "Epoch 5/30\n",
      "55/55 [==============================] - 75s 1s/step - loss: 0.8842 - accuracy: 0.6435 - val_loss: 0.8733 - val_accuracy: 0.6556\n",
      "Epoch 6/30\n",
      "31/55 [===============>..............] - ETA: 25s - loss: 0.8193 - accuracy: 0.6754"
     ]
    }
   ],
   "source": [
    "final_model.fit(X_train, y_train, validation_data = (X_cv, y_cv), \n",
    "               epochs = 30, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc4c75d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
